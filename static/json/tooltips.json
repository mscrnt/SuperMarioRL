{
    "n_steps": {
        "title": "Number of Steps",
        "description": "Number of steps to run per environment per update. Must be greater than 1 to enable advantage normalization.",
        "example": "Example: Set n_steps to 2048 for standard PPO training.",
        "proTip": "Higher values increase stability but require more memory. Recommended to keep between 1024 and 8192."
    },
    "batch_size": {
        "title": "Batch Size",
        "description": "Size of the mini-batches used for training. The dropdown menu dynamically calculates valid batch sizes based on the total rollout size (n_steps Ã— num_envs). Only values that evenly divide the total rollout are displayed.",
        "example": "Example: Use powers of 2 (e.g., 64, 128) when possible for optimal GPU utilization.",
        "proTip": "Larger batch sizes reduce variance but require more memory. Use the dropdown menu to select the largest valid size for faster training while balancing memory usage."
    },
    "n_epochs": {
        "title": "Number of Epochs",
        "description": "Number of passes through the data during optimization. Higher values improve convergence but increase computational cost.",
        "example": "Example: Set this between 3 and 10.",
        "proTip": "If loss fluctuates, increase the epochs for smoother convergence."
    },
    "gamma": {
        "title": "Discount Factor",
        "description": "Controls how much the model values long-term rewards. Range is 0 to 1.",
        "example": "Example: Use 0.99 for long-term tasks.",
        "proTip": "Keep this close to 1 for tasks that require planning or learning from distant rewards."
    },
    "gae_lambda": {
        "title": "GAE Lambda",
        "description": "Bias-variance trade-off parameter for the Generalized Advantage Estimation.",
        "example": "Example: Set to 0.95 for balanced bias and variance.",
        "proTip": "Lower values reduce bias, higher values reduce variance."
    },
    "clip_range_start": {
        "title": "Initial Clip Range",
        "description": "The starting value for the clipping range in PPO updates. This limits how much the policy can change during each update, stabilizing training.",
        "example": "Example: Start with 0.2 for most environments.",
        "proTip": "Use 0.2 as a baseline and adjust downward if training is unstable. Valid range: 0 to 1."
    },
    "clip_range_end": {
        "title": "Final Clip Range",
        "description": "The final value for the clipping range in PPO updates. This value is used at the end of training and helps fine-tune policy updates.",
        "example": "Example: End with 0.05 for refined updates.",
        "proTip": "Smaller values, such as 0.05, prevent overfitting during the final stages of training. Valid range: 0 to 1. Ensure this value is not higher than the initial clip range."
    },
    "vf_coef": {
        "title": "Value Function Coefficient",
        "description": "Weight for the value function loss. Balances policy and value updates.",
        "example": "Example: Default is 0.5.",
        "proTip": "Increase if the value function is lagging behind policy updates."
    },
    "ent_coef": {
        "title": "Entropy Coefficient",
        "description": "Coefficient for entropy regularization to encourage exploration.",
        "example": "Example: Set to a small value like 0.01.",
        "proTip": "Reduce if entropy loss is too high; increase to encourage exploration."
    },
    "max_grad_norm": {
        "title": "Max Gradient Norm",
        "description": "Maximum value for gradient clipping to prevent exploding gradients.",
        "example": "Example: Default is 0.5.",
        "proTip": "Set to a smaller value if training becomes unstable."
    },
    "normalize_advantage": {
        "title": "Normalize Advantage",
        "description": "Whether to normalize advantages for training.",
        "example": "Example: Set to True to improve stability.",
        "proTip": "Leave this as True unless using a custom setup."
    },
    "device": {
        "title": "Device",
        "description": "Device to run the code on ('cpu', 'cuda', or 'auto').",
        "example": "Example: Use 'auto' to default to GPU when available.",
        "proTip": "Set to 'cuda' for faster training if a GPU is available."
    },
    "num_envs": {
        "title": "Number of Environments",
        "description": "Number of parallel environments to use for training.",
        "example": "Example: Use 8 or 16 for faster training.",
        "proTip": "Increase for faster training but higher memory usage."
    },
    "total_timesteps": {
        "title": "Total Timesteps",
        "description": "Total number of timesteps to train the model.",
        "example": "Example: Set to 1e6 (1,000,000) for medium-sized tasks.",
        "proTip": "Set this to a large value for better performance."
    },
    "autosave_freq": {
        "title": "Autosave Frequency",
        "description": "Frequency to save the model during training.",
        "example": "Example: Save every 100,000 timesteps.",
        "proTip": "Set this to a small value to avoid losing progress."
    },
    "random_stages": {
        "title": "Random Stages",
        "description": "Train on random stages from the environment.",
        "example": "Example: Use True for diversity in training.",
        "proTip": "Enable this for better generalization across levels."
    },
    "stages": {
        "title": "Stages",
        "description": "List of stages to train on. Works only if random_stages is True.",
        "example": "Example: ['1-1', '2-1', '3-1']",
        "proTip": "Specify key stages for focused training."
    },
    "wrappers": {
        "title": "Wrappers",
        "description": "Wrappers modify the environment by adding custom functionality like observation filtering, reward shaping, or additional logging.",
        "example": "Example: Use a wrapper to normalize observations:\n```python\nfrom stable_baselines3.common.vec_env import VecNormalize\nenv = VecNormalize(env)\n```",
        "proTip": "Use wrappers for preprocessing tasks like observation normalization or action clipping to simplify training."
    },
    "callbacks": {
        "title": "Callbacks",
        "description": "Callbacks allow you to hook into the training process for events like logging, saving models, or early stopping.",
        "example": "Example: Save the model periodically using CheckpointCallback:\n```python\nfrom stable_baselines3.common.callbacks import CheckpointCallback\ncallback = CheckpointCallback(save_freq=1000, save_path='./models/')\n```",
        "proTip": "Combine multiple callbacks using `CallbackList` for complex workflows, like saving, logging, and custom metrics."
    },
    "learning_rate_start": {
        "title": "Learning Rate (Start)",
        "description": "The initial learning rate for the optimizer, determining how fast the model learns at the start of training.",
        "example": "Example: Start with 0.0003 for stable learning.",
        "proTip": "Use a moderate value like 0.0003 to avoid divergence. Valid range: 0 to 1."
    },
    "learning_rate_end": {
        "title": "Learning Rate (End)",
        "description": "The final learning rate for the optimizer, used towards the end of training for fine-tuning.",
        "example": "Example: End with 0.00005 for precise adjustments.",
        "proTip": "Smaller values help with fine-tuning and ensure stability in later stages. Valid range: 0 to 1. Ensure this value is not higher than the starting learning rate."
    },
    "clip_range_vf_start": {
        "title": "Value Function Clip Range (Start)",
        "description": "Initial clipping range for value function updates. Limits how much the value function can change during updates to maintain stability.",
        "example": "Example: Start with 0.2 for typical tasks.",
        "proTip": "If value function loss fluctuates too much, adjust this value. Leave as 'None' if clipping is unnecessary. Valid range: 0 to 1."
    },
    "clip_range_vf_end": {
        "title": "Value Function Clip Range (End)",
        "description": "Final clipping range for value function updates, applied during the last stages of training.",
        "example": "Example: End with 0.05 to refine value function updates.",
        "proTip": "Lower final values stabilize the value function during fine-tuning. Valid range: 0 to 1. Ensure this value is not higher than the initial value function clip range."
    },
    "pi_net": {
        "title": "Policy Network Architecture",
        "description": "Defines the architecture of the policy network. The network layers are specified as comma-separated integers.",
        "example": "Example: Use '256,256' for two layers with 256 neurons each.",
        "proTip": "Adjust based on the complexity of the environment. Larger networks handle more complex policies but require more memory and computation."
    },
    "vf_net": {
        "title": "Value Function Network Architecture",
        "description": "Defines the architecture of the value function network. The network layers are specified as comma-separated integers.",
        "example": "Example: Use '256,256' for two layers with 256 neurons each.",
        "proTip": "Match the architecture of the policy network for balanced training. Increase size for tasks with complex value functions."
    },
    "training_config": {
        "title": "Training Configuration",
        "description": "Defines high-level training parameters, such as the number of environments, stages, and total timesteps for training. Used to control the overall behavior of the training loop.",
        "example": "Example: { 'num_envs': 4, 'total_timesteps': 1e7, 'stages': ['level1', 'level2'] }",
        "proTip": "Adjust 'num_envs' based on available CPU cores to maximize training throughput."
    },
    "hyperparameters": {
        "title": "Hyperparameters",
        "description": "Specifies low-level training parameters that control optimization, value updates, and agent behavior. Includes values such as learning rate, batch size, and discount factors.",
        "example": "Example: { 'learning_rate': 3e-4, 'batch_size': 64, 'gamma': 0.99, 'clip_range': 0.2 }",
        "proTip": "Tuning hyperparameters can greatly impact training performance. Start with default values and adjust based on training stability and reward trends."
    },
    "rollout_buffer_class": {
        "title": "Rollout Buffer Class",
        "description": "Specifies the buffer class used to store and sample rollout data during training. Defaults to the appropriate class for the selected algorithm if left None.",
        "example": "Example: Use 'DictRolloutBuffer' for environments with dictionary observation spaces.",
        "proTip": "Customize this value only if you need advanced buffer behavior or specific sampling strategies."
    },
    "rollout_buffer_kwargs": {
        "title": "Rollout Buffer Keyword Arguments",
        "description": "Additional configuration options for the rollout buffer. Accepts a dictionary of arguments passed during buffer initialization.",
        "example": "Example: { 'gamma': 0.99, 'gae_lambda': 0.95 } for Generalized Advantage Estimation.",
        "proTip": "Use this to fine-tune the behavior of the buffer, such as adjusting gamma or enabling time-limited discounts."
    },
    "target_kl": {
        "title": "Target KL Divergence",
        "description": "Limits the KL divergence between updates to prevent excessively large policy changes. Useful for maintaining stable training.",
        "example": "Example: Set 'target_kl': 0.01 to limit KL divergence during updates.",
        "proTip": "Set this value to None if you want no KL divergence constraints. Use small values like 0.01 to enforce tighter stability."
    }
}
